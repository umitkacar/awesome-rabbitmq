<div align="center">

# ğŸ¤– AI Agents with RabbitMQ

### _The Complete Guide to Building Production-Ready AI Agent Systems with Message Queues_

[![RabbitMQ](https://img.shields.io/badge/RabbitMQ-FF6600?style=for-the-badge&logo=rabbitmq&logoColor=white)](https://www.rabbitmq.com/)
[![AI](https://img.shields.io/badge/AI_Agents-00D9FF?style=for-the-badge&logo=openai&logoColor=white)](https://openai.com/)
[![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://www.python.org/)
[![LangChain](https://img.shields.io/badge/LangChain-121212?style=for-the-badge&logo=chainlink&logoColor=white)](https://langchain.com/)

<p align="center">
  <a href="#-introduction">Introduction</a> â€¢
  <a href="#-architecture-patterns">Architecture</a> â€¢
  <a href="#-agent-frameworks">Frameworks</a> â€¢
  <a href="#-production-examples">Examples</a> â€¢
  <a href="#-best-practices">Best Practices</a>
</p>

</div>

---

## ğŸ“‹ Table of Contents

- [ğŸ¯ Introduction](#-introduction)
  - [Why RabbitMQ for AI Agents?](#why-rabbitmq-for-ai-agents)
  - [Key Benefits](#key-benefits)
  - [When to Use This Pattern](#when-to-use-this-pattern)
- [ğŸ—ï¸ Architecture Patterns](#ï¸-architecture-patterns)
  - [Single Agent Queue Pattern](#single-agent-queue-pattern)
  - [Multi-Agent Coordination](#multi-agent-coordination)
  - [Agent Swarm Architecture](#agent-swarm-architecture)
  - [Event-Driven Agent Systems](#event-driven-agent-systems)
- [ğŸ”§ Agent Frameworks Integration](#-agent-frameworks-integration)
  - [LangChain & LangGraph](#langchain--langgraph)
  - [AutoGen](#autogen)
  - [CrewAI](#crewai)
  - [Custom Agent Implementation](#custom-agent-implementation)
- [ğŸ“š Task Distribution & Queue Management](#-task-distribution--queue-management)
- [ğŸ’» Production-Ready Examples](#-production-ready-examples)
- [âš¡ Performance & Scaling](#-performance--scaling)
- [ğŸ”’ Security & Best Practices](#-security--best-practices)
- [ğŸ“Š Monitoring & Observability](#-monitoring--observability)
- [ğŸŒŸ Real-World Case Studies](#-real-world-case-studies)
- [ğŸš€ Advanced Topics](#-advanced-topics)
- [ğŸ“– Resources](#-resources)

---

## ğŸ¯ Introduction

### Why RabbitMQ for AI Agents?

AI agents are autonomous software entities that can perceive their environment, make decisions, and take actions to achieve specific goals. When building production AI agent systems, **reliable message queuing** becomes essential for:

- **Asynchronous Processing**: AI operations (LLM calls, embeddings, tool execution) can take seconds to minutes
- **Load Distribution**: Balance workload across multiple agent instances
- **Fault Tolerance**: Retry failed operations without data loss
- **Scalability**: Add more agent workers dynamically based on queue depth
- **Decoupling**: Separate agent logic from application code

**RabbitMQ** provides the perfect foundation for building these systems with:
- âœ… Message persistence and delivery guarantees
- âœ… Flexible routing and exchange patterns
- âœ… Priority queues for urgent requests
- âœ… Dead letter queues for error handling
- âœ… Built-in clustering and high availability

### Key Benefits

| Benefit | Description | Impact |
|---------|-------------|--------|
| ğŸ”„ **Async Operations** | Agents process tasks without blocking | 10x higher throughput |
| ğŸ“Š **Load Balancing** | Distribute tasks across agent instances | Linear scalability |
| ğŸ›¡ï¸ **Reliability** | Message acknowledgments prevent data loss | 99.99% uptime |
| ğŸ¯ **Priority Handling** | Critical tasks processed first | Better UX |
| ğŸ” **Observability** | Queue metrics show system health | Faster debugging |
| ğŸ’° **Cost Efficiency** | Only pay for active agents | 60% cost reduction |

### When to Use This Pattern

#### âœ… Perfect For:

- **LLM-Powered Applications**: ChatGPT, Claude, Gemini API calls
- **Multi-Agent Systems**: Specialized agents working together
- **RAG Pipelines**: Document processing, embedding generation, retrieval
- **Tool-Using Agents**: Agents that call external APIs
- **Long-Running Workflows**: Multi-step agent chains
- **Background Processing**: Email analysis, document generation
- **Real-Time Analytics**: Processing streaming data with AI

#### âŒ Not Ideal For:

- Simple synchronous AI calls (use direct API)
- Single-user applications with low volume
- Sub-second latency requirements (use Redis)
- When message ordering is critical across all messages

---

## ğŸ—ï¸ Architecture Patterns

### Single Agent Queue Pattern

**Use Case**: Deploy AI model as a microservice with queue-based load balancing.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   API/Web    â”‚      â”‚   RabbitMQ   â”‚      â”‚ AI Agent 1   â”‚
â”‚   Server     â”‚â”€â”€â”€â”€â”€â–¶â”‚    Queue     â”‚â”€â”€â”€â”€â”€â–¶â”‚ (Worker)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ AI Agent 2   â”‚
                                            â”‚ (Worker)     â”‚
                                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                            â”‚ AI Agent N   â”‚
                                           â–¶â”‚ (Worker)     â”‚
                                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Implementation**:

```python
# producer.py - API Server
import pika
import json
import uuid

class AgentTaskProducer:
    def __init__(self, rabbitmq_url='amqp://localhost'):
        self.connection = pika.BlockingConnection(pika.URLParameters(rabbitmq_url))
        self.channel = self.connection.channel()

        # Declare queue with priority support
        self.channel.queue_declare(
            queue='ai_agent_tasks',
            durable=True,
            arguments={
                'x-max-priority': 10,
                'x-message-ttl': 3600000,  # 1 hour TTL
                'x-dead-letter-exchange': 'dlx',
                'x-dead-letter-routing-key': 'failed_tasks'
            }
        )

    def submit_task(self, task_type, payload, priority=5):
        """Submit a task to the AI agent queue"""
        task_id = str(uuid.uuid4())

        message = {
            'task_id': task_id,
            'task_type': task_type,
            'payload': payload,
            'timestamp': time.time()
        }

        self.channel.basic_publish(
            exchange='',
            routing_key='ai_agent_tasks',
            body=json.dumps(message),
            properties=pika.BasicProperties(
                delivery_mode=2,  # persistent
                priority=priority,
                correlation_id=task_id,
                content_type='application/json'
            )
        )

        return task_id

# consumer.py - AI Agent Worker
import pika
import json
from openai import OpenAI

class AIAgentWorker:
    def __init__(self, rabbitmq_url='amqp://localhost', model='gpt-4'):
        self.client = OpenAI()
        self.model = model

        self.connection = pika.BlockingConnection(pika.URLParameters(rabbitmq_url))
        self.channel = self.connection.channel()

        # Set QoS - process one task at a time
        self.channel.basic_qos(prefetch_count=1)

    def process_task(self, task):
        """Process different types of AI tasks"""
        task_type = task['task_type']
        payload = task['payload']

        if task_type == 'chat_completion':
            return self.chat_completion(payload)
        elif task_type == 'text_analysis':
            return self.text_analysis(payload)
        elif task_type == 'summarization':
            return self.summarize(payload)
        else:
            raise ValueError(f"Unknown task type: {task_type}")

    def chat_completion(self, payload):
        """Handle chat completion requests"""
        response = self.client.chat.completions.create(
            model=self.model,
            messages=payload['messages'],
            temperature=payload.get('temperature', 0.7),
            max_tokens=payload.get('max_tokens', 1000)
        )
        return response.choices[0].message.content

    def callback(self, ch, method, properties, body):
        """Process messages from queue"""
        try:
            task = json.loads(body)
            print(f"Processing task {task['task_id']}: {task['task_type']}")

            # Process the task
            result = self.process_task(task)

            # Publish result to results queue
            result_message = {
                'task_id': task['task_id'],
                'status': 'completed',
                'result': result,
                'timestamp': time.time()
            }

            ch.basic_publish(
                exchange='',
                routing_key='ai_agent_results',
                body=json.dumps(result_message),
                properties=pika.BasicProperties(
                    correlation_id=properties.correlation_id
                )
            )

            # Acknowledge message
            ch.basic_ack(delivery_tag=method.delivery_tag)
            print(f"Task {task['task_id']} completed successfully")

        except Exception as e:
            print(f"Error processing task: {e}")
            # Reject and requeue (will go to DLQ after max retries)
            ch.basic_nack(delivery_tag=method.delivery_tag, requeue=False)

    def start(self):
        """Start consuming tasks"""
        self.channel.basic_consume(
            queue='ai_agent_tasks',
            on_message_callback=self.callback
        )

        print('ğŸ¤– AI Agent Worker started. Waiting for tasks...')
        self.channel.start_consuming()

# Usage
if __name__ == '__main__':
    worker = AIAgentWorker()
    worker.start()
```

### Multi-Agent Coordination

**Use Case**: Different specialized agents handle different task types.

```
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚   Task Router    â”‚
                        â”‚   (Exchange)     â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                    â”‚                    â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
    â”‚ Research Q   â”‚    â”‚ Analysis Q   â”‚    â”‚ Writing Q    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
            â”‚                    â”‚                    â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
    â”‚ Research     â”‚    â”‚ Analysis     â”‚    â”‚ Writing      â”‚
    â”‚ Agent        â”‚    â”‚ Agent        â”‚    â”‚ Agent        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Implementation**:

```python
# multi_agent_system.py
import pika
import json
from typing import Dict, Callable
from openai import OpenAI

class MultiAgentSystem:
    def __init__(self, rabbitmq_url='amqp://localhost'):
        self.connection = pika.BlockingConnection(pika.URLParameters(rabbitmq_url))
        self.channel = self.connection.channel()
        self.client = OpenAI()

        # Declare topic exchange for routing
        self.channel.exchange_declare(
            exchange='agent_tasks',
            exchange_type='topic',
            durable=True
        )

        # Agent-specific system prompts
        self.system_prompts = {
            'research': "You are a research specialist. Gather comprehensive information on topics.",
            'analysis': "You are an analytical expert. Analyze data and provide insights.",
            'writing': "You are a professional writer. Create clear, engaging content.",
            'coding': "You are a senior software engineer. Write clean, efficient code."
        }

    def declare_agent_queue(self, agent_type: str, routing_key: str):
        """Declare queue for specific agent type"""
        queue_name = f'{agent_type}_agent_queue'

        self.channel.queue_declare(
            queue=queue_name,
            durable=True,
            arguments={
                'x-max-priority': 10,
                'x-dead-letter-exchange': 'dlx'
            }
        )

        # Bind queue to exchange with routing key
        self.channel.queue_bind(
            exchange='agent_tasks',
            queue=queue_name,
            routing_key=routing_key
        )

        return queue_name

    def create_agent_worker(self, agent_type: str):
        """Create a specialized agent worker"""
        queue_name = self.declare_agent_queue(agent_type, f'agent.{agent_type}.*')

        def callback(ch, method, properties, body):
            try:
                task = json.loads(body)
                print(f"[{agent_type.upper()}] Processing: {task['task_id']}")

                # Execute with agent-specific system prompt
                response = self.client.chat.completions.create(
                    model='gpt-4',
                    messages=[
                        {"role": "system", "content": self.system_prompts[agent_type]},
                        {"role": "user", "content": task['prompt']}
                    ],
                    temperature=task.get('temperature', 0.7)
                )

                result = {
                    'task_id': task['task_id'],
                    'agent_type': agent_type,
                    'result': response.choices[0].message.content,
                    'status': 'completed'
                }

                # Publish result
                ch.basic_publish(
                    exchange='agent_results',
                    routing_key=f'result.{agent_type}',
                    body=json.dumps(result)
                )

                ch.basic_ack(delivery_tag=method.delivery_tag)

            except Exception as e:
                print(f"[{agent_type.upper()}] Error: {e}")
                ch.basic_nack(delivery_tag=method.delivery_tag, requeue=False)

        self.channel.basic_qos(prefetch_count=1)
        self.channel.basic_consume(queue=queue_name, on_message_callback=callback)

        print(f'ğŸ¤– {agent_type.upper()} Agent started')
        self.channel.start_consuming()

# Orchestrator
class AgentOrchestrator:
    def __init__(self, rabbitmq_url='amqp://localhost'):
        self.connection = pika.BlockingConnection(pika.URLParameters(rabbitmq_url))
        self.channel = self.connection.channel()

    def dispatch_task(self, agent_type: str, task_id: str, prompt: str, priority=5):
        """Dispatch task to specific agent type"""
        routing_key = f'agent.{agent_type}.task'

        message = {
            'task_id': task_id,
            'prompt': prompt,
            'timestamp': time.time()
        }

        self.channel.basic_publish(
            exchange='agent_tasks',
            routing_key=routing_key,
            body=json.dumps(message),
            properties=pika.BasicProperties(
                delivery_mode=2,
                priority=priority
            )
        )

        print(f"Dispatched task {task_id} to {agent_type} agent")

# Usage
if __name__ == '__main__':
    import sys

    if len(sys.argv) < 2:
        print("Usage: python multi_agent_system.py [research|analysis|writing|coding]")
        sys.exit(1)

    agent_type = sys.argv[1]
    system = MultiAgentSystem()
    system.create_agent_worker(agent_type)
```

### Agent Swarm Architecture

**Use Case**: Large-scale parallel processing with many agents working on sub-tasks.

```python
# agent_swarm.py
import pika
import json
import time
from concurrent.futures import ThreadPoolExecutor
from openai import OpenAI

class AgentSwarm:
    """
    Agent Swarm for parallel task processing
    Inspired by ant colony optimization - many simple agents working together
    """

    def __init__(self, rabbitmq_url='amqp://localhost', swarm_size=10):
        self.rabbitmq_url = rabbitmq_url
        self.swarm_size = swarm_size
        self.client = OpenAI()

    def setup_infrastructure(self):
        """Setup RabbitMQ infrastructure for swarm"""
        connection = pika.BlockingConnection(pika.URLParameters(self.rabbitmq_url))
        channel = connection.channel()

        # Main task distribution queue
        channel.queue_declare(
            queue='swarm_tasks',
            durable=True,
            arguments={'x-max-priority': 10}
        )

        # Results aggregation queue
        channel.queue_declare(queue='swarm_results', durable=True)

        # Coordination exchange for swarm communication
        channel.exchange_declare(
            exchange='swarm_coordination',
            exchange_type='fanout'
        )

        connection.close()

    def swarm_worker(self, worker_id: int):
        """Individual agent in the swarm"""
        connection = pika.BlockingConnection(pika.URLParameters(self.rabbitmq_url))
        channel = connection.channel()
        channel.basic_qos(prefetch_count=1)

        def callback(ch, method, properties, body):
            task = json.loads(body)

            try:
                print(f"[Worker-{worker_id}] Processing chunk {task['chunk_id']}")

                # Process the sub-task
                response = self.client.chat.completions.create(
                    model='gpt-4',
                    messages=[
                        {"role": "system", "content": task['system_prompt']},
                        {"role": "user", "content": task['content']}
                    ]
                )

                result = {
                    'chunk_id': task['chunk_id'],
                    'parent_task_id': task['parent_task_id'],
                    'worker_id': worker_id,
                    'result': response.choices[0].message.content,
                    'processing_time': time.time() - task['timestamp']
                }

                # Publish result
                ch.basic_publish(
                    exchange='',
                    routing_key='swarm_results',
                    body=json.dumps(result)
                )

                ch.basic_ack(delivery_tag=method.delivery_tag)

            except Exception as e:
                print(f"[Worker-{worker_id}] Error: {e}")
                ch.basic_nack(delivery_tag=method.delivery_tag, requeue=True)

        channel.basic_consume(queue='swarm_tasks', on_message_callback=callback)
        print(f'ğŸœ Swarm Worker-{worker_id} started')
        channel.start_consuming()

    def start_swarm(self):
        """Launch all swarm workers"""
        self.setup_infrastructure()

        with ThreadPoolExecutor(max_workers=self.swarm_size) as executor:
            for worker_id in range(self.swarm_size):
                executor.submit(self.swarm_worker, worker_id)

# Task Splitter - breaks large tasks into chunks for swarm
class SwarmTaskSplitter:
    def __init__(self, rabbitmq_url='amqp://localhost'):
        self.connection = pika.BlockingConnection(pika.URLParameters(rabbitmq_url))
        self.channel = self.connection.channel()

    def split_and_distribute(self, task_id: str, content: str, chunk_size=1000):
        """Split large task into chunks and distribute to swarm"""

        # Split content into chunks
        chunks = [content[i:i+chunk_size] for i in range(0, len(content), chunk_size)]

        print(f"Splitting task {task_id} into {len(chunks)} chunks")

        for idx, chunk in enumerate(chunks):
            message = {
                'chunk_id': f"{task_id}_chunk_{idx}",
                'parent_task_id': task_id,
                'content': chunk,
                'total_chunks': len(chunks),
                'chunk_index': idx,
                'system_prompt': "Analyze this text chunk and provide insights.",
                'timestamp': time.time()
            }

            self.channel.basic_publish(
                exchange='',
                routing_key='swarm_tasks',
                body=json.dumps(message),
                properties=pika.BasicProperties(delivery_mode=2)
            )

        print(f"Distributed {len(chunks)} chunks to swarm")

# Result Aggregator
class SwarmResultAggregator:
    def __init__(self, rabbitmq_url='amqp://localhost'):
        self.connection = pika.BlockingConnection(pika.URLParameters(rabbitmq_url))
        self.channel = self.connection.channel()
        self.results = {}

    def aggregate_results(self, parent_task_id: str, expected_chunks: int):
        """Collect and aggregate results from swarm workers"""

        def callback(ch, method, properties, body):
            result = json.loads(body)

            if result['parent_task_id'] == parent_task_id:
                chunk_id = result['chunk_id']
                self.results[chunk_id] = result

                print(f"Received result {len(self.results)}/{expected_chunks}")

                if len(self.results) >= expected_chunks:
                    # All chunks processed
                    print("All chunks processed! Aggregating...")
                    self.finalize_results(parent_task_id)
                    ch.basic_ack(delivery_tag=method.delivery_tag)
                    ch.stop_consuming()
                else:
                    ch.basic_ack(delivery_tag=method.delivery_tag)

        self.channel.basic_consume(
            queue='swarm_results',
            on_message_callback=callback
        )

        print(f"Waiting for {expected_chunks} results...")
        self.channel.start_consuming()

    def finalize_results(self, task_id: str):
        """Combine all chunk results"""
        sorted_results = sorted(
            self.results.values(),
            key=lambda x: int(x['chunk_id'].split('_')[-1])
        )

        combined = "\n\n".join([r['result'] for r in sorted_results])

        avg_time = sum(r['processing_time'] for r in sorted_results) / len(sorted_results)

        print(f"\n{'='*60}")
        print(f"Task {task_id} completed!")
        print(f"Total chunks: {len(sorted_results)}")
        print(f"Average processing time: {avg_time:.2f}s")
        print(f"{'='*60}\n")
        print(combined)

# Usage Example
if __name__ == '__main__':
    import sys

    if len(sys.argv) > 1 and sys.argv[1] == 'swarm':
        # Start swarm workers
        swarm = AgentSwarm(swarm_size=5)
        swarm.start_swarm()
    else:
        # Submit a large task
        splitter = SwarmTaskSplitter()

        large_document = """
        [Your large document here - will be split across swarm]
        """ * 100  # Simulate large document

        task_id = "task_" + str(int(time.time()))
        splitter.split_and_distribute(task_id, large_document)

        # Wait for results
        aggregator = SwarmResultAggregator()
        aggregator.aggregate_results(task_id, expected_chunks=5)
```

### Event-Driven Agent Systems

**Pattern**: Agents react to events and trigger other agents.

```python
# event_driven_agents.py
import pika
import json
from dataclasses import dataclass
from typing import List, Dict
from openai import OpenAI

@dataclass
class AgentEvent:
    event_type: str
    source_agent: str
    payload: Dict
    timestamp: float

class EventDrivenAgent:
    """Base class for event-driven agents"""

    def __init__(self, agent_name: str, rabbitmq_url='amqp://localhost'):
        self.agent_name = agent_name
        self.client = OpenAI()
        self.connection = pika.BlockingConnection(pika.URLParameters(rabbitmq_url))
        self.channel = self.connection.channel()

        # Declare events exchange
        self.channel.exchange_declare(
            exchange='agent_events',
            exchange_type='topic',
            durable=True
        )

        # Agent's input queue
        result = self.channel.queue_declare(queue='', exclusive=True)
        self.queue_name = result.method.queue

    def subscribe_to_events(self, event_patterns: List[str]):
        """Subscribe to specific event patterns"""
        for pattern in event_patterns:
            self.channel.queue_bind(
                exchange='agent_events',
                queue=self.queue_name,
                routing_key=pattern
            )
            print(f"[{self.agent_name}] Subscribed to: {pattern}")

    def publish_event(self, event_type: str, payload: Dict):
        """Publish an event for other agents"""
        event = AgentEvent(
            event_type=event_type,
            source_agent=self.agent_name,
            payload=payload,
            timestamp=time.time()
        )

        routing_key = f"event.{event_type}"

        self.channel.basic_publish(
            exchange='agent_events',
            routing_key=routing_key,
            body=json.dumps(event.__dict__)
        )

        print(f"[{self.agent_name}] Published event: {event_type}")

    def handle_event(self, event: AgentEvent):
        """Override this method in subclasses"""
        raise NotImplementedError

    def start(self):
        """Start listening for events"""
        def callback(ch, method, properties, body):
            event_data = json.loads(body)
            event = AgentEvent(**event_data)

            if event.source_agent != self.agent_name:  # Don't process own events
                print(f"[{self.agent_name}] Received event: {event.event_type}")
                self.handle_event(event)

            ch.basic_ack(delivery_tag=method.delivery_tag)

        self.channel.basic_consume(
            queue=self.queue_name,
            on_message_callback=callback
        )

        print(f'ğŸ¤– {self.agent_name} started and listening for events...')
        self.channel.start_consuming()

# Specialized Agent Implementations

class DocumentAnalyzerAgent(EventDrivenAgent):
    """Analyzes documents and publishes insights"""

    def __init__(self):
        super().__init__('DocumentAnalyzer')
        self.subscribe_to_events(['event.document.uploaded'])

    def handle_event(self, event: AgentEvent):
        if event.event_type == 'document.uploaded':
            document_id = event.payload['document_id']
            content = event.payload['content']

            # Analyze document with LLM
            response = self.client.chat.completions.create(
                model='gpt-4',
                messages=[
                    {"role": "system", "content": "Analyze this document and extract key insights."},
                    {"role": "user", "content": content}
                ]
            )

            analysis = response.choices[0].message.content

            # Publish analysis event
            self.publish_event('document.analyzed', {
                'document_id': document_id,
                'analysis': analysis,
                'sentiment': 'positive'  # Simplified
            })

class SummarizerAgent(EventDrivenAgent):
    """Creates summaries when documents are analyzed"""

    def __init__(self):
        super().__init__('Summarizer')
        self.subscribe_to_events(['event.document.analyzed'])

    def handle_event(self, event: AgentEvent):
        if event.event_type == 'document.analyzed':
            analysis = event.payload['analysis']

            # Create summary
            response = self.client.chat.completions.create(
                model='gpt-4',
                messages=[
                    {"role": "system", "content": "Create a concise summary (max 100 words)."},
                    {"role": "user", "content": analysis}
                ]
            )

            summary = response.choices[0].message.content

            # Publish summary event
            self.publish_event('summary.created', {
                'document_id': event.payload['document_id'],
                'summary': summary
            })

class NotificationAgent(EventDrivenAgent):
    """Sends notifications when summaries are ready"""

    def __init__(self):
        super().__init__('Notifier')
        self.subscribe_to_events(['event.summary.created'])

    def handle_event(self, event: AgentEvent):
        if event.event_type == 'summary.created':
            summary = event.payload['summary']

            # Send notification (simplified)
            print(f"\n{'='*60}")
            print(f"ğŸ“§ NOTIFICATION: Summary Ready!")
            print(f"Document: {event.payload['document_id']}")
            print(f"Summary: {summary}")
            print(f"{'='*60}\n")

            # Publish notification sent event
            self.publish_event('notification.sent', {
                'document_id': event.payload['document_id'],
                'channel': 'email'
            })

# Event Publisher (simulates document upload)
class EventPublisher:
    def __init__(self, rabbitmq_url='amqp://localhost'):
        self.connection = pika.BlockingConnection(pika.URLParameters(rabbitmq_url))
        self.channel = self.connection.channel()

        self.channel.exchange_declare(
            exchange='agent_events',
            exchange_type='topic',
            durable=True
        )

    def trigger_document_upload(self, document_id: str, content: str):
        """Simulate document upload event"""
        event = AgentEvent(
            event_type='document.uploaded',
            source_agent='System',
            payload={
                'document_id': document_id,
                'content': content
            },
            timestamp=time.time()
        )

        self.channel.basic_publish(
            exchange='agent_events',
            routing_key='event.document.uploaded',
            body=json.dumps(event.__dict__)
        )

        print(f"ğŸ“„ Document {document_id} uploaded!")

# Usage
if __name__ == '__main__':
    import sys

    if len(sys.argv) > 1:
        agent_type = sys.argv[1]

        if agent_type == 'analyzer':
            agent = DocumentAnalyzerAgent()
            agent.start()
        elif agent_type == 'summarizer':
            agent = SummarizerAgent()
            agent.start()
        elif agent_type == 'notifier':
            agent = NotificationAgent()
            agent.start()
        else:
            print("Unknown agent type")
    else:
        # Trigger an event
        publisher = EventPublisher()
        publisher.trigger_document_upload(
            'doc_123',
            "This is a sample document about AI agents using RabbitMQ for event-driven architecture..."
        )
```

---

## ğŸ”§ Agent Frameworks Integration

### LangChain & LangGraph

**Integration Pattern**: Use RabbitMQ for task distribution to LangChain agents.

```python
# langchain_rabbitmq_agent.py
import pika
import json
from langchain.agents import AgentExecutor, create_openai_functions_agent
from langchain_openai import ChatOpenAI
from langchain.tools import Tool
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder

class LangChainRabbitMQAgent:
    """LangChain agent powered by RabbitMQ task queue"""

    def __init__(self, rabbitmq_url='amqp://localhost'):
        # Setup RabbitMQ
        self.connection = pika.BlockingConnection(pika.URLParameters(rabbitmq_url))
        self.channel = self.connection.channel()
        self.channel.queue_declare(queue='langchain_tasks', durable=True)
        self.channel.basic_qos(prefetch_count=1)

        # Setup LangChain Agent
        self.llm = ChatOpenAI(model="gpt-4", temperature=0)
        self.tools = self.create_tools()
        self.agent = self.create_agent()

    def create_tools(self):
        """Define tools for the agent"""
        def search_documents(query: str) -> str:
            """Search through documents"""
            # Implement your search logic
            return f"Found documents matching: {query}"

        def calculate(expression: str) -> str:
            """Perform calculations"""
            try:
                result = eval(expression)
                return f"Result: {result}"
            except:
                return "Invalid expression"

        return [
            Tool(
                name="SearchDocuments",
                func=search_documents,
                description="Search through document database"
            ),
            Tool(
                name="Calculator",
                func=calculate,
                description="Perform mathematical calculations"
            )
        ]

    def create_agent(self):
        """Create LangChain agent with tools"""
        prompt = ChatPromptTemplate.from_messages([
            ("system", "You are a helpful AI assistant with access to tools."),
            ("human", "{input}"),
            MessagesPlaceholder(variable_name="agent_scratchpad")
        ])

        agent = create_openai_functions_agent(self.llm, self.tools, prompt)
        return AgentExecutor(agent=agent, tools=self.tools, verbose=True)

    def process_task(self, task):
        """Execute LangChain agent on task"""
        result = self.agent.invoke({"input": task['query']})
        return result['output']

    def start(self):
        """Start consuming tasks from RabbitMQ"""
        def callback(ch, method, properties, body):
            try:
                task = json.loads(body)
                print(f"Processing task: {task['task_id']}")

                # Execute LangChain agent
                result = self.process_task(task)

                # Publish result
                result_message = {
                    'task_id': task['task_id'],
                    'result': result,
                    'status': 'completed'
                }

                ch.basic_publish(
                    exchange='',
                    routing_key='langchain_results',
                    body=json.dumps(result_message)
                )

                ch.basic_ack(delivery_tag=method.delivery_tag)

            except Exception as e:
                print(f"Error: {e}")
                ch.basic_nack(delivery_tag=method.delivery_tag, requeue=False)

        self.channel.basic_consume(
            queue='langchain_tasks',
            on_message_callback=callback
        )

        print('ğŸ¦œ LangChain Agent started with RabbitMQ')
        self.channel.start_consuming()

# Usage
if __name__ == '__main__':
    agent = LangChainRabbitMQAgent()
    agent.start()
```

### AutoGen

**Integration**: Multi-agent conversations with RabbitMQ coordination.

```python
# autogen_rabbitmq.py
import pika
import json
from autogen import AssistantAgent, UserProxyAgent, GroupChat, GroupChatManager

class AutoGenRabbitMQOrchestrator:
    """Orchestrate AutoGen agents with RabbitMQ"""

    def __init__(self, rabbitmq_url='amqp://localhost'):
        self.connection = pika.BlockingConnection(pika.URLParameters(rabbitmq_url))
        self.channel = self.connection.channel()

        # Setup AutoGen agents
        self.setup_agents()

    def setup_agents(self):
        """Initialize AutoGen agents"""
        llm_config = {
            "model": "gpt-4",
            "temperature": 0.7
        }

        self.engineer = AssistantAgent(
            name="Engineer",
            llm_config=llm_config,
            system_message="You are a software engineer. Write clean, efficient code."
        )

        self.reviewer = AssistantAgent(
            name="CodeReviewer",
            llm_config=llm_config,
            system_message="You are a code reviewer. Provide constructive feedback."
        )

        self.user_proxy = UserProxyAgent(
            name="UserProxy",
            human_input_mode="NEVER",
            max_consecutive_auto_reply=5
        )

        # Create group chat
        self.group_chat = GroupChat(
            agents=[self.user_proxy, self.engineer, self.reviewer],
            messages=[],
            max_round=10
        )

        self.manager = GroupChatManager(groupchat=self.group_chat, llm_config=llm_config)

    def process_task(self, task_description: str):
        """Execute AutoGen conversation for task"""
        self.user_proxy.initiate_chat(
            self.manager,
            message=task_description
        )

        # Extract final result
        conversation_history = self.group_chat.messages
        return conversation_history

# Integration with RabbitMQ similar to previous examples
```

### CrewAI

**Integration**: Crew of AI agents working together via RabbitMQ.

```python
# crewai_rabbitmq.py
from crewai import Agent, Task, Crew
import pika
import json

class CrewAIRabbitMQIntegration:
    """CrewAI agents coordinated by RabbitMQ"""

    def __init__(self, rabbitmq_url='amqp://localhost'):
        self.connection = pika.BlockingConnection(pika.URLParameters(rabbitmq_url))
        self.channel = self.connection.channel()

        # Setup CrewAI
        self.setup_crew()

    def setup_crew(self):
        """Initialize CrewAI agents and tasks"""

        # Define agents
        researcher = Agent(
            role='Research Analyst',
            goal='Gather comprehensive information',
            backstory='Expert researcher with deep analytical skills',
            verbose=True
        )

        writer = Agent(
            role='Content Writer',
            goal='Create engaging content',
            backstory='Professional writer with storytelling expertise',
            verbose=True
        )

        # Define tasks
        research_task = Task(
            description='Research the topic thoroughly',
            agent=researcher
        )

        writing_task = Task(
            description='Write comprehensive article',
            agent=writer
        )

        # Create crew
        self.crew = Crew(
            agents=[researcher, writer],
            tasks=[research_task, writing_task],
            verbose=True
        )

    def execute_crew_task(self, topic: str):
        """Execute crew on a topic"""
        result = self.crew.kickoff(inputs={'topic': topic})
        return result

# Similar RabbitMQ integration pattern
```

---

## ğŸ“š Task Distribution & Queue Management

### Fair Scheduling for LLM Agents

```python
# fair_scheduler.py
import pika
import json
from collections import defaultdict
from queue import Queue

class FairScheduler:
    """
    Implement fair scheduling for multi-user LLM agent system
    Each user gets round-robin access to agent resources
    """

    def __init__(self, rabbitmq_url='amqp://localhost'):
        self.connection = pika.BlockingConnection(pika.URLParameters(rabbitmq_url))
        self.channel = self.connection.channel()

        # User-specific queues
        self.user_queues = defaultdict(lambda: Queue())
        self.user_queue_names = {}

    def setup_user_queue(self, user_id: str, priority='normal'):
        """Create dedicated queue for user with priority"""
        queue_name = f'user_{user_id}_queue'

        priority_value = {'high': 10, 'normal': 5, 'low': 1}[priority]

        self.channel.queue_declare(
            queue=queue_name,
            durable=True,
            arguments={
                'x-max-priority': 10,
                'x-default-priority': priority_value
            }
        )

        # Bind to main exchange with user routing
        self.channel.queue_bind(
            exchange='fair_scheduler',
            queue=queue_name,
            routing_key=f'user.{user_id}'
        )

        self.user_queue_names[user_id] = queue_name
        return queue_name

    def submit_user_task(self, user_id: str, task: dict, priority='normal'):
        """Submit task to user-specific queue"""
        if user_id not in self.user_queue_names:
            self.setup_user_queue(user_id, priority)

        routing_key = f'user.{user_id}'

        self.channel.basic_publish(
            exchange='fair_scheduler',
            routing_key=routing_key,
            body=json.dumps(task),
            properties=pika.BasicProperties(delivery_mode=2)
        )

class RoundRobinAgent:
    """Agent that processes tasks in round-robin fashion across users"""

    def __init__(self, rabbitmq_url='amqp://localhost'):
        self.connection = pika.BlockingConnection(pika.URLParameters(rabbitmq_url))
        self.channel = self.connection.channel()

        # Get all user queues
        self.user_queues = self.get_user_queues()
        self.current_queue_idx = 0

    def get_user_queues(self):
        """Discover all user-specific queues"""
        # In production, maintain a registry
        return ['user_alice_queue', 'user_bob_queue', 'user_charlie_queue']

    def process_round_robin(self):
        """Process one task from each user queue in round-robin"""
        for queue_name in self.user_queues:
            method, properties, body = self.channel.basic_get(queue=queue_name)

            if method:
                task = json.loads(body)
                print(f"Processing task from {queue_name}: {task['task_id']}")

                # Process task
                result = self.execute_task(task)

                # Acknowledge
                self.channel.basic_ack(method.delivery_tag)

                return result

        return None

    def execute_task(self, task):
        """Execute the actual task"""
        # Your LLM processing logic
        pass
```

### Priority-Based Queue Management

```python
# priority_queue_manager.py
class PriorityQueueManager:
    """
    Manage multiple priority levels for AI agent tasks
    Interactive > Batch > Background
    """

    PRIORITY_LEVELS = {
        'interactive': 10,  # Chat applications - need fast response
        'api': 7,           # API calls - moderate priority
        'batch': 3,         # Batch processing - can wait
        'background': 1     # Background jobs - lowest priority
    }

    def __init__(self, rabbitmq_url='amqp://localhost'):
        self.connection = pika.BlockingConnection(pika.URLParameters(rabbitmq_url))
        self.channel = self.connection.channel()
        self.setup_queues()

    def setup_queues(self):
        """Setup priority queues"""
        self.channel.queue_declare(
            queue='ai_tasks',
            durable=True,
            arguments={'x-max-priority': 10}
        )

    def submit_task(self, task: dict, task_type: str = 'api'):
        """Submit task with appropriate priority"""
        priority = self.PRIORITY_LEVELS.get(task_type, 5)

        self.channel.basic_publish(
            exchange='',
            routing_key='ai_tasks',
            body=json.dumps(task),
            properties=pika.BasicProperties(
                delivery_mode=2,
                priority=priority,
                headers={'task_type': task_type}
            )
        )

        print(f"Submitted {task_type} task with priority {priority}")
```

---

## âš¡ Performance & Scaling

### Horizontal Scaling Pattern

```yaml
# docker-compose.yml for scaled AI agents
version: '3.8'

services:
  rabbitmq:
    image: rabbitmq:4.2-management
    ports:
      - "5672:5672"
      - "15672:15672"
    environment:
      RABBITMQ_DEFAULT_USER: admin
      RABBITMQ_DEFAULT_PASS: admin
    healthcheck:
      test: ["CMD", "rabbitmq-diagnostics", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  ai_agent:
    build: ./ai_agent
    depends_on:
      rabbitmq:
        condition: service_healthy
    environment:
      RABBITMQ_URL: amqp://admin:admin@rabbitmq:5672
      OPENAI_API_KEY: ${OPENAI_API_KEY}
    deploy:
      replicas: 5  # Scale to 5 agent workers
      resources:
        limits:
          cpus: '1'
          memory: 2G

  # Auto-scaling based on queue depth
  scaler:
    image: custom/queue-scaler
    environment:
      RABBITMQ_URL: amqp://admin:admin@rabbitmq:5672
      SCALE_UP_THRESHOLD: 100    # Scale up if queue > 100
      SCALE_DOWN_THRESHOLD: 10   # Scale down if queue < 10
      MAX_AGENTS: 20
      MIN_AGENTS: 2
```

### Performance Metrics

```python
# metrics_collector.py
import pika
import time
from prometheus_client import start_http_server, Counter, Histogram, Gauge

class AgentMetricsCollector:
    """Collect and expose Prometheus metrics for AI agents"""

    def __init__(self):
        # Define metrics
        self.tasks_processed = Counter(
            'agent_tasks_processed_total',
            'Total tasks processed by agents',
            ['agent_type', 'status']
        )

        self.task_duration = Histogram(
            'agent_task_duration_seconds',
            'Task processing duration',
            ['agent_type']
        )

        self.queue_depth = Gauge(
            'agent_queue_depth',
            'Current queue depth',
            ['queue_name']
        )

        self.llm_tokens = Counter(
            'agent_llm_tokens_total',
            'Total LLM tokens used',
            ['model', 'type']  # type: prompt/completion
        )

    def record_task_completion(self, agent_type: str, duration: float, status: str):
        """Record task completion metrics"""
        self.tasks_processed.labels(agent_type=agent_type, status=status).inc()
        self.task_duration.labels(agent_type=agent_type).observe(duration)

    def update_queue_depth(self, queue_name: str, depth: int):
        """Update queue depth metric"""
        self.queue_depth.labels(queue_name=queue_name).set(depth)

    def record_llm_usage(self, model: str, prompt_tokens: int, completion_tokens: int):
        """Record LLM token usage"""
        self.llm_tokens.labels(model=model, type='prompt').inc(prompt_tokens)
        self.llm_tokens.labels(model=model, type='completion').inc(completion_tokens)

# Start metrics server
metrics = AgentMetricsCollector()
start_http_server(8000)  # Expose metrics on :8000/metrics
```

---

## ğŸ”’ Security & Best Practices

### Secure Agent Configuration

```python
# secure_agent.py
import pika
from pika.credentials import ExternalCredentials
import ssl

class SecureAIAgent:
    """AI Agent with TLS and authentication"""

    def __init__(self, rabbitmq_host: str, cert_path: str, key_path: str):
        # TLS/SSL Configuration
        context = ssl.create_default_context(cafile="/path/to/ca_cert.pem")
        context.load_cert_chain(cert_path, key_path)

        ssl_options = pika.SSLOptions(context, rabbitmq_host)

        # Connection parameters with TLS
        parameters = pika.ConnectionParameters(
            host=rabbitmq_host,
            port=5671,  # TLS port
            ssl_options=ssl_options,
            credentials=ExternalCredentials()
        )

        self.connection = pika.BlockingConnection(parameters)
        self.channel = self.connection.channel()
```

### Best Practices Checklist

âœ… **Message Security**
- Encrypt sensitive data before sending to queue
- Use TLS for all RabbitMQ connections
- Implement proper authentication and authorization

âœ… **Error Handling**
- Always use message acknowledgments
- Implement dead letter queues for failed tasks
- Add retry logic with exponential backoff

âœ… **Resource Management**
- Set `prefetch_count` to limit concurrent tasks per worker
- Monitor memory usage of agent processes
- Implement circuit breakers for external API calls

âœ… **Monitoring**
- Track queue depth and processing rates
- Monitor LLM token usage and costs
- Set up alerts for queue buildup

âœ… **Cost Optimization**
- Cache frequently accessed embeddings
- Batch similar requests when possible
- Use cheaper models for simple tasks

---

## ğŸ“Š Monitoring & Observability

### Complete Monitoring Stack

```yaml
# monitoring-docker-compose.yml
version: '3.8'

services:
  prometheus:
    image: prom/prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus

  grafana:
    image: grafana/grafana
    ports:
      - "3000:3000"
    environment:
      GF_SECURITY_ADMIN_PASSWORD: admin
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/dashboards:/etc/grafana/provisioning/dashboards

  rabbitmq:
    image: rabbitmq:4.2-management
    ports:
      - "5672:5672"
      - "15672:15672"
      - "15692:15692"  # Prometheus metrics
    environment:
      RABBITMQ_PROMETHEUS_PLUGIN_ENABLED: "true"

volumes:
  prometheus_data:
  grafana_data:
```

```yaml
# prometheus.yml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'rabbitmq'
    static_configs:
      - targets: ['rabbitmq:15692']

  - job_name: 'ai_agents'
    static_configs:
      - targets: ['ai_agent:8000']
```

---

## ğŸŒŸ Real-World Case Studies

### Case Study 1: Large-Scale Document Processing

**Company**: Financial Services Firm
**Challenge**: Process 100K+ financial documents daily with AI analysis
**Solution**: RabbitMQ + AI Agent Swarm

**Architecture**:
- 50 AI agent workers (auto-scaling 10-100)
- Document chunks distributed via RabbitMQ
- Results aggregated in real-time
- 99.9% uptime SLA

**Results**:
- âœ… Processing time: 2 hours â†’ 15 minutes
- âœ… Cost reduction: 40% (vs direct API calls)
- âœ… Handles peak loads of 500 documents/minute

### Case Study 2: Customer Support AI

**Company**: E-commerce Platform
**Challenge**: Handle 10K support tickets/day with AI triage
**Solution**: Multi-Agent System with Priority Queues

**Architecture**:
- Priority queues (VIP, Standard, Low)
- Specialized agents (Refunds, Technical, General)
- RabbitMQ routing based on ticket type

**Results**:
- âœ… 80% tickets auto-resolved
- âœ… Response time: 2 hours â†’ 5 minutes
- âœ… Customer satisfaction: +35%

---

## ğŸš€ Advanced Topics

### Circuit Breaker Pattern for LLM APIs

```python
# circuit_breaker.py
from enum import Enum
import time

class CircuitState(Enum):
    CLOSED = "closed"      # Normal operation
    OPEN = "open"          # Failing, don't try
    HALF_OPEN = "half_open"  # Testing recovery

class CircuitBreaker:
    """Prevent cascading failures from LLM API issues"""

    def __init__(self, failure_threshold=5, timeout=60):
        self.failure_threshold = failure_threshold
        self.timeout = timeout
        self.failures = 0
        self.last_failure_time = None
        self.state = CircuitState.CLOSED

    def call(self, func, *args, **kwargs):
        """Execute function with circuit breaker protection"""

        if self.state == CircuitState.OPEN:
            if time.time() - self.last_failure_time > self.timeout:
                self.state = CircuitState.HALF_OPEN
            else:
                raise Exception("Circuit breaker is OPEN")

        try:
            result = func(*args, **kwargs)
            self.on_success()
            return result
        except Exception as e:
            self.on_failure()
            raise e

    def on_success(self):
        """Reset on successful call"""
        self.failures = 0
        self.state = CircuitState.CLOSED

    def on_failure(self):
        """Increment failures and open circuit if threshold reached"""
        self.failures += 1
        self.last_failure_time = time.time()

        if self.failures >= self.failure_threshold:
            self.state = CircuitState.OPEN
            print(f"âš ï¸ Circuit breaker OPEN after {self.failures} failures")
```

### Rate Limiting for API Costs

```python
# rate_limiter.py
import time
from collections import deque

class TokenBucketRateLimiter:
    """Rate limit LLM API calls to control costs"""

    def __init__(self, rate_limit=100, time_window=60):
        """
        rate_limit: max requests per time_window
        time_window: time window in seconds
        """
        self.rate_limit = rate_limit
        self.time_window = time_window
        self.requests = deque()

    def allow_request(self):
        """Check if request is allowed under rate limit"""
        now = time.time()

        # Remove old requests outside time window
        while self.requests and self.requests[0] < now - self.time_window:
            self.requests.popleft()

        if len(self.requests) < self.rate_limit:
            self.requests.append(now)
            return True

        return False

    def wait_time(self):
        """Calculate wait time until next request allowed"""
        if not self.requests:
            return 0

        oldest = self.requests[0]
        return max(0, self.time_window - (time.time() - oldest))
```

---

## ğŸ“– Resources

### Official Documentation
- [RabbitMQ Official Docs](https://www.rabbitmq.com/docs)
- [LangChain Documentation](https://docs.langchain.com/)
- [AutoGen Documentation](https://microsoft.github.io/autogen/)
- [CrewAI Documentation](https://docs.crewai.com/)

### Tutorials & Guides
- [RabbitMQ Python Tutorial](https://www.rabbitmq.com/tutorials/tutorial-one-python)
- [Building AI Agents](https://platform.openai.com/docs/guides/agents)
- [Event-Driven Architecture with RabbitMQ](https://blog.theodo.com/2019/08/event-driven-architectures-rabbitmq/)

### Tools & Libraries
- **Celery**: Distributed task queue for Python
- **Dramatiq**: Fast distributed task processing
- **Temporal**: Workflow orchestration platform
- **Dapr**: Distributed application runtime

### Community Resources
- [Awesome LLM Agents](https://github.com/topics/llm-agents)
- [RabbitMQ Community](https://github.com/rabbitmq)
- [AI Engineering Community](https://www.latent.space/)

---

<div align="center">

### ğŸ¯ Key Takeaways

**RabbitMQ provides the perfect backbone for production AI agent systems:**

âœ… Reliable message delivery for expensive LLM operations
âœ… Flexible routing for multi-agent coordination
âœ… Scalability to handle varying loads
âœ… Cost optimization through efficient queuing
âœ… Battle-tested in production environments

**Start building scalable AI agent systems today!**

[â¬†ï¸ Back to Top](#-ai-agents-with-rabbitmq)

---

**Part of the [Awesome Message Brokers](../README.md) collection | [RabbitMQ Guide](README.md)**

</div>
